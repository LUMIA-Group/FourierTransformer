# random stringsssssssssssssssssssssssss
config = {
    "listops":{
        "dataset":{
            "train":96000,
            "dev":2000,
            "test":2000,
        },
        "model":{
            "learn_pos_emb":True,
            "tied_weights":False,
            "embedding_dim":512,
            "transformer_dim":512,
            "transformer_hidden_dim":2048,
            "head_dim":64,
            "num_head":8,
            "num_layers":6,
            "vocab_size":32,
            "max_seq_len":2000,
            "dropout_prob":0.1,
            "attention_dropout":0.1,
            "pooling_mode":"CLS",
            "num_classes":10,
        },
        "training":{
            "batch_size":64,
            "learning_rate":0.0001,
            "warmup":96000//64*20//10,
            "lr_decay":"linear",
            "weight_decay":1e-1,
            "eval_frequency":500,
            "num_train_steps":96000//64*20,
            "num_eval_steps":2000//64,
        },
        "gpu_memory":64,
        "extra_attn_config":{
            "softmax":{"attention_grad_checkpointing":True},
            "dct-0.2":{"attention_grad_checkpointing":False, "dct_percentage":0.2},
        }
    },
    "image":{
        "dataset":{
            "train":45000,
            "dev":5000,
            "test":10000,
        },
        "model":{
            "learn_pos_emb":True,
            "tied_weights":False,
            "embedding_dim":64,
            "transformer_dim":64,
            "transformer_hidden_dim":128,
            "head_dim":16,
            "num_head":4,
            "num_layers":3,
            "vocab_size":256,
            "max_seq_len":1024,
            "dropout_prob":0.3,
            "attention_dropout":0.3,
            "pooling_mode":"CLS",
            "num_classes": 10,
        },
        "training":{
            "batch_size":256,
            "learning_rate":0.0018,
            "warmup":45000//256*150//10,
            "lr_decay":"linear",
            "weight_decay":0,
            "eval_frequency":500,
            "num_train_steps":45000//256*150,
            "num_eval_steps":5000//256,
        },
        "gpu_memory":256,
        "extra_attn_config":{
            "softmax":{"attention_grad_checkpointing":True},
            "dct-0.2":{"attention_grad_checkpointing":False, "dct_percentage":0.2},
        }
    },
    "pathfinder32":{
        "dataset":{
            "train":160000,
            "dev":20000,
            "test":20000,
        },
        "model":{
            "learn_pos_emb":True,
            "tied_weights":False,
            "embedding_dim":128,
            "transformer_dim":128,
            "transformer_hidden_dim":128,
            "head_dim":16,
            "num_head":8,
            "num_layers":4,
            "vocab_size":512,
            "max_seq_len":1024,
            "dropout_prob":0.3,
            "attention_dropout":0.3,
            "pooling_mode":"CLS",
            "num_classes": 2,
        },
        "training":{
            "batch_size":512,
            "learning_rate":0.0005,
            "warmup":160000//512*200//10,
            "lr_decay":"linear",
            "weight_decay":0,
            "eval_frequency":500,
            "num_train_steps":160000//512*200,
            "num_eval_steps":20000//512,
        },
        "gpu_memory":512,
        "extra_attn_config":{
            "softmax":{"attention_grad_checkpointing":True},
            "dct-0.2":{"attention_grad_checkpointing":False, "dct_percentage":0.2},
        }
    },
    "retrieval":{
        "dataset":{
            "train":147086,
            "dev":18090,
            "test":17437,
        },
        "model":{
            "learn_pos_emb":True,
            "tied_weights":False,
            "embedding_dim":128,
            "transformer_dim":128,
            "transformer_hidden_dim":512,
            "head_dim":32,
            "num_head":4,
            "num_layers":4,
            "vocab_size":512,
            "max_seq_len":1000,
            "dropout_prob":0.1,
            "attention_dropout":0.1,
            "pooling_mode":"CLS",
            "num_classes":2,
        },
        "training":{
            "batch_size":64,
            "learning_rate":0.001,
            "warmup":147086//64*10//10,
            "lr_decay":"linear",
            "weight_decay":0,
            "eval_frequency":1000,
            "num_train_steps":147086//64*10,
            "num_eval_steps":18090//64,
        },
        "gpu_memory":64,
        "extra_attn_config":{
            "softmax":{"attention_grad_checkpointing":True},
            "dct-0.2":{"attention_grad_checkpointing":False, "dct_percentage":0.2},
        }
    },
    "text":{
        "dataset":{
            "train":25000,
            "dev":25000,
            "test":25000,
        },
        "model":{
            "learn_pos_emb":True,
            "tied_weights":False,
            "embedding_dim":512,
            "transformer_dim":512,
            "transformer_hidden_dim":2048,
            "head_dim":64,
            "num_head":8,
            "num_layers":6,
            "vocab_size":512,
            "max_seq_len":2000,
            "dropout_prob":0.1,
            "attention_dropout":0.1,
            "pooling_mode":"CLS",
            "num_classes": 2,
        },
        "training":{
            "batch_size":64,
            "learning_rate":0.00002,
            "warmup":25000//64*20//10,
            "lr_decay":"linear",
            "weight_decay":0,
            "eval_frequency":30,
            "num_train_steps":25000//64*20,
            "num_eval_steps":25000//64,
        },
        "gpu_memory":64,
        "extra_attn_config":{
            "softmax":{"attention_grad_checkpointing":True},
            "dct-0.2":{"attention_grad_checkpointing":False, "dct_percentage":0.2},
            "dct-0.8":{"attention_grad_checkpointing":False, "dct_percentage":0.8},

        }
    }
}

config["pathfinder32-curv_baseline"] = config["pathfinder32"]
config["pathfinder32-curv_contour_length_9"] = config["pathfinder32"]
config["pathfinder32-curv_contour_length_14"] = config["pathfinder32"]
